# ignore_header_test
# Copyright 2023 Stanford University
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

work_directory: "."

scheduler:
<<<<<<< Updated upstream
  lr: 0.0002568168330296152 #optimize LSTM #optimize TRANSFORMER
  lr_decay: 0.008033642156496669 #optimize LSTM #optimize TRANSFORMER

training:
  batch_size: 20 #optimize LSTM #optimize AE
=======
  lr: 0.0002268032935778 #optimize LSTM #optimize TRANSFORMER
  lr_decay: 0.04454193231964785 #optimize LSTM #optimize TRANSFORMER

training:
  batch_size: 43 #optimize LSTM #optimize AE
>>>>>>> Stashed changes
  epochs: 500
  geometries: "test" #"healthy" for LSTM, "test" for AE --------------------
  train_test_split: 0.9
  loss_weight_boundary_nodes: 37 #optimize LSTM
  output_interval: 50
  backward_interval: 5 

checkpoints:
  ckpt_path: "checkpoints"
  ckpt_name: "model.pt"

performance:
  amp: False
  jit: False

testing:
  graph: "s0095_0001.21.0.grph"

architecture:
  hidden_dim: 58 #optimize LSTM
  in_feats: 17 #put 18 for LSTM and 17 for AE --------------------
  edge_feats: 9
<<<<<<< Updated upstream
  latent_size_gnn: 36 #optimize LSTM #optimize AE
  latent_size_mlp: 193 #optimize LSTM #optimize AE
  number_hidden_layers_mlp: 2 #optimize LSTM #optimize AE
  out_size: 2 
  autoloop_iterations: 1 #optimize LSTM
  process_iterations: 2 #optimize LSTM #optimize AE
  latent_size_AE: 87 #optimize AE
=======
  latent_size_gnn: 35 #optimize LSTM #optimize AE
  latent_size_mlp: 159 #optimize LSTM #optimize AE
  number_hidden_layers_mlp: 1 #optimize LSTM #optimize AE
  out_size: 2 
  autoloop_iterations: 1 #optimize LSTM
  process_iterations: 0 #optimize LSTM #optimize AE
  latent_size_AE: 3 #optimize AE
>>>>>>> Stashed changes

hyperparameter_optimization:
  runs: 100

model:
  LSTM: "False" # "True" for LSTM model, "False" for Transformer --------------------


transformer_architecture:
<<<<<<< Updated upstream
=======
  threshold: 1000000.0
  N_heads: 4 #hpo Transformer
>>>>>>> Stashed changes
  N_timesteps: 41
  N_inn: 10 #hpo Transformer
  N_g: 4 # hpo Transformer
  N_neu_MLP_p: 30 # hpo Transformer
  N_hid_MLP_p: 3  # hpo Transformer
  N_neu_MLP_m: 30 # hpo Transformer
  N_hid_MLP_m: 3  # hpo Transformer
  checkpoints_ckpt_path: "checkpoints_transformer"
  checkpoints_ckpt_name: "model.pt"