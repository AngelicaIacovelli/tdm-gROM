# ignore_header_test
# Copyright 2023 Stanford University
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

work_directory: "."

scheduler:
  lr: 0.0111093 #optimize LSTM #optimize AE + TRANSFORMER
  lr_decay: 0.0540696 #optimize LSTM #optimize AE + TRANSFORMER

training:
  batch_size: 10 #optimize LSTM #optimize AE
  epochs: 100
  geometries: "test" #"healthy" for LSTM, "test" for AE --------------------
  train_test_split: 0.9
  loss_weight_boundary_nodes: 37 #optimize LSTM
  output_interval: 50
  backward_interval: 5 

checkpoints:
  ckpt_path: "checkpoints"
  ckpt_name: "model.pt"

performance:
  amp: False
  jit: False

testing:
  graph: "s0095_0001.21.0.grph"

architecture:
  hidden_dim: 58 #optimize LSTM
  in_feats: 17 #put 18 for LSTM and 17 for AE --------------------
  edge_feats: 9
  latent_size_gnn: 12 #optimize LSTM #optimize AE
  latent_size_mlp: 178 #optimize LSTM #optimize AE
  number_hidden_layers_mlp: 1 #optimize LSTM #optimize AE
  out_size: 2 
  autoloop_iterations: 1 #optimize LSTM
  process_iterations: 0 #optimize LSTM #optimize AE
  latent_size_AE: 41 #optimize AE

hyperparameter_optimization:
  runs: 100

model:
  LSTM: "False" # "True" for LSTM model, "False" for Transformer --------------------


transformer_architecture:
  epochs: 10000
  threshold: 1000000.0
  N_heads: 3 #hpo Transformer
  N_timesteps: 41
  N_inn: 48 #hpo Transformer
  N_g: 21 # hpo Transformer
  N_neu_MLP_p: 6 # hpo Transformer
  N_hid_MLP_p: 5  # hpo Transformer
  N_neu_MLP_m: 13 # hpo Transformer
  N_hid_MLP_m: 4  # hpo Transformer
  incremental_loss: "False"
  checkpoints_ckpt_path: "checkpoints_transformer"
  checkpoints_ckpt_name: "model.pt"