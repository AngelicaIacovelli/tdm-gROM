# ignore_header_test
# Copyright 2023 Stanford University
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

work_directory: "."

scheduler:
  lr: 0.00043727270420871694 #optimize LSTM #optimize TRANSFORMER
  lr_decay: 0.028826399523639707 #optimize LSTM #optimize TRANSFORMER

training:
  batch_size: 27 #optimize LSTM #optimize AE
  epochs: 500
  geometries: "healthy" #"healthy" for LSTM, "test" for AE
  train_test_split: 0.9
  loss_weight_boundary_nodes: 37 #optimize LSTM
  output_interval: 50
  backward_interval: 5 

checkpoints:
  ckpt_path: "checkpoints"
  ckpt_name: "model.pt"

performance:
  amp: False
  jit: False

testing:
  graph: "s0095_0001.21.0.grph"

architecture:
  hidden_dim: 58 #optimize LSTM
  in_feats: 18 #put 18 for LSTM and 17 for AE
  edge_feats: 9
  latent_size_gnn: 26 #optimize LSTM #optimize AE
  latent_size_mlp: 134 #optimize LSTM #optimize AE
  number_hidden_layers_mlp: 2 #optimize LSTM #optimize AE
  out_size: 2 
  autoloop_iterations: 1 #optimize LSTM
  process_iterations: 1 #optimize LSTM #optimize AE
  latent_size_AE: 10 #optimize AE

hyperparameter_optimization:
  runs: 100

model:
  LSTM: "True" # "True" for LSTM model, "False" for Transformer 


transformer_architecture:
  N_t: 41
  N_lat: 60 #why not hpo? 
  N_inn: 10 #hpo Transformer
  N_g: 4 # hpo Transformer
  N_mu: 30 #hpo Transformer
  N_neu_MLP_p: 30 # hpo Transformer
  N_hid_MLP_p: 3  # hpo Transformer
  N_neu_MLP_m: 30 # hpo Transformer
  N_hid_MLP_m: 3  # hpo Transformer
 