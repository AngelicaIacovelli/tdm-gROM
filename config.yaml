# ignore_header_test
# Copyright 2023 Stanford University
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

work_directory: "."

hyperparameter_optimization:
  runs: 100
  flag: "False" # "True" for Transformer hpo, "False" otherwise --------------------

model:
  LSTM: "True" # "True" for LSTM model, "False" for Transformer --------------------

performance:
  amp: False
  jit: False

testing:
  graph: "s0095_0001.21.0.grph"

# LSTM and AE 

scheduler:
  lr: 0.00020325566074545003 #optimize LSTM #optimize AE 
  lr_decay: 0.003746192435115448 #optimize LSTM #optimize AE 

training:
  batch_size: 10 #optimize LSTM #optimize AE
  epochs: 500
  geometries: "healthy" #"healthy" for LSTM, "test" or "one_geometry" for AE --------------------
  train_test_split: 0.9
  loss_weight_boundary_nodes: 37 #optimize LSTM
  output_interval: 50
  backward_interval: 5 

checkpoints:
  ckpt_path: "checkpoints"
  ckpt_name: "model.pt"

architecture:
  hidden_dim: 58 #optimize LSTM
  in_feats: 18 #put 18 for LSTM and 17 for AE --------------------
  edge_feats: 9
  latent_size_gnn: 12 #optimize LSTM #optimize AE
  latent_size_mlp: 178 #optimize LSTM #optimize AE
  number_hidden_layers_mlp: 1 #optimize LSTM #optimize AE
  out_size: 2 
  autoloop_iterations: 1 #optimize LSTM
  process_iterations: 0 #optimize LSTM #optimize AE
  latent_size_AE: 41 #optimize AE


# TRANSFORMER

transformer_architecture:
  lr: 0.000117216
  lr_decay: 0.00571391
  epochs: 100
  threshold: 1000000.0
  N_heads: 2 #hpo Transformer
  N_timesteps: 41
  N_inn: 5 #hpo Transformer
  N_g: 10 # hpo Transformer
  N_neu_MLP_p: 10 # hpo Transformer
  N_hid_MLP_p: 4  # hpo Transformer
  N_neu_MLP_m: 16 # hpo Transformer
  N_hid_MLP_m: 4  # hpo Transformer
  incremental_loss: "False"
  checkpoints_ckpt_path: "checkpoints_transformer"
  checkpoints_ckpt_name: "model.pt"